{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traverse root directory, and list directories as dirs and files as files\n",
    "for root, dirs, files in os.walk(\".\"):\n",
    "    path = root.split(os.sep)\n",
    "    print((len(path) - 1) * '---', os.path.basename(root))\n",
    "    for file in files:\n",
    "        print(len(path) * '---', file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaggle datasets\n",
    "metadt = pd.read_csv('./datasets/movies_metadata.csv', low_memory=False)\n",
    "metadt.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender system using weighted rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Penggunaan rating untuk rekomendasi memiliki kekurangan:  \n",
    "* rating tidak memberikan gambaran popularitas produk. Produk A memilki rating 9.5 dari 20 voters sedangkan produk B memiliki rating 8.7 dari 1000 voters. Mana yang lebih baik? tentu saja produk B, rating yang diberikan oleh lebih banyak user lebih terpercaya dibandingkan rating tinggi dengan sedikit user\n",
    "  \n",
    "Oleh karena itu, perlu dilakukan pembobotan rating sebagaimana rumus dibawah ini:  \n",
    "\\begin{equation}\n",
    "\\text Weighted Rating (\\bf WR) = \\left({{\\bf v} \\over {\\bf v} + {\\bf m}} \\cdot R\\right) + \\left({{\\bf m} \\over {\\bf v} + {\\bf m}} \\cdot C\\right)\n",
    "\\end{equation}  \n",
    "Keterangan:  \n",
    "v = jumlah voters  \n",
    "m = minimum votes yang dibutuhkan untuk masuk dalam list  \n",
    "R = rata-rata rating  \n",
    "C = mean atau rata-rata vote secara keseluruhan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average rating in datasets\n",
    "C = metadt.vote_average.mean()\n",
    "print(C)\n",
    "\n",
    "# calculate minimum number of vote (m)\n",
    "# here i'm gonna using 90% percentile\n",
    "m = metadt.vote_count.quantile(0.9)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter movie with vote_count more than m\n",
    "top_movies = metadt.copy().loc[metadt['vote_count'] > m]\n",
    "print('shape:', metadt.shape)\n",
    "print('shape:', top_movies.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_rating(data, m=m, C=C):\n",
    "    v = data['vote_count']\n",
    "    R = data['vote_average']\n",
    "    return (v/(v+m) * R) + (m/(v+m) * C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_movies['weighted_rating'] = top_movies.apply(weighted_rating, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_movies = top_movies.sort_values('weighted_rating', ascending=False)\n",
    "print('shape:', top_movies.shape)\n",
    "top_movies[['title', 'vote_count', 'vote_average', 'weighted_rating']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content-based Recommender\n",
    "Natural Language Processing (TF-IDF) using **overview** feature on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# create TF-IDF object and remove all stop_word like 'and', 'or', 'the', etc.\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "metadt['overview'] = metadt['overview'].fillna('')\n",
    "tfidf_matrix = tfidf.fit_transform(metadt['overview'])\n",
    "\n",
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selanjutnya hitung nilai similarity. Score similarity dapat ditentukan menggunakan manhattan distance, euclidean distance, pearson atau cosine similarity. Berikut merupakan rumus untuk menghitung cosine similarity:\n",
    "<img src='https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1590782185/cos_aalkpq.png'/>\n",
    "  \n",
    "kita akan menggunakan sklearn **linear_kernel()** karena lebih cepat dibandingkan **cosine_similarity()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "cos_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "print(cos_sim.shape)\n",
    "cos_sim[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse mapping title and index for recommender function\n",
    "indices = pd.Series(metadt.index, index=metadt['title']).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendation(title, cos_sim=cos_sim):\n",
    "    ind = indices[title] # find index for title\n",
    "    sim = list(enumerate(cos_sim[ind])) # enumerate all cosine similarity for the title\n",
    "    sim = sorted(sim, key=lambda x: x[1], reverse=True) # sorted by second column (cosine similarity)\n",
    "    sim = sim[1:10] # get top 10 highest similarity\n",
    "    movie_indices = [ x[0] for x in sim ]\n",
    "    return metadt['title'].loc[movie_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_recommendation('The Shawshank Redemption')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add more feature recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credits = pd.read_csv('./datasets/credits.csv')\n",
    "keywords = pd.read_csv('./datasets/keywords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkInteger(data):\n",
    "    try:\n",
    "        int(data)\n",
    "    except ValueError:\n",
    "        return True\n",
    "\n",
    "bad_id = [x for x in metadt.id if checkInteger(x)]\n",
    "print(bad_id)\n",
    "index_id = metadt.loc[metadt['id'].isin(bad_id)].index\n",
    "metadt.drop(index_id, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords['id'] = keywords['id'].astype('int')\n",
    "credits['id'] = credits['id'].astype('int')\n",
    "metadt['id'] = metadt['id'].astype('int')\n",
    "\n",
    "#merge keywords and credits to metadt\n",
    "metadt = metadt.merge(keywords, on='id')\n",
    "metadt = metadt.merge(credits, on='id')\n",
    "metadt.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stringified list\n",
    "from ast import literal_eval\n",
    "\n",
    "features = ['cast', 'crew', 'keywords', 'genres']\n",
    "for feature in features:\n",
    "    metadt[feature] = metadt[feature].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_director(x):\n",
    "    for i in x:\n",
    "        if i['job'] == 'Director':\n",
    "            return i['name']\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top 3 from instance (crews, keywords, genres)\n",
    "def get_list(x):\n",
    "    if isinstance(x, list):\n",
    "        names = [i['name'] for i in x]\n",
    "        if len(names) > 3:\n",
    "            names = names[:3]\n",
    "        return names\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply get_director and get_list\n",
    "metadt['director'] = metadt['crew'].apply(get_director)\n",
    "\n",
    "features = ['cast', 'keywords', 'genres']\n",
    "for feature in features:\n",
    "    metadt[feature] = metadt[feature].apply(get_list)\n",
    "    \n",
    "metadt[['title', 'cast', 'director', 'keywords', 'genres']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip and lower case all string\n",
    "def clean_data(x):\n",
    "    if isinstance(x, list):\n",
    "        return [str.lower(i.replace(' ', '')) for i in x]\n",
    "    else:\n",
    "        if isinstance(x, str):\n",
    "            return str.lower(x.replace(' ', ''))\n",
    "        else:\n",
    "            return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply clean data\n",
    "features = ['cast', 'keywords', 'director', 'genres']\n",
    "\n",
    "for feature in features:\n",
    "    metadt[feature] = metadt[feature].apply(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all data needed to string\n",
    "def create_soup(x):\n",
    "    return ' '.join(x['keywords']) + ' ' + ' '.join(x['cast']) + ' ' + x['director'] + ' ' + ' '.join(x['genres'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply soup\n",
    "metadt['soup'] = metadt.apply(create_soup, axis=1)\n",
    "metadt[['soup']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommending section\n",
    "# Director, genre, cast on relatively more movies doesn't affect their presence\n",
    "# so we used CountVectorizer instead of TF-IDF\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count = CountVectorizer(stop_words='english')\n",
    "count_matrix = count.fit_transform(metadt['soup'])\n",
    "count_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure distance using cosine_similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cos_sim2 = cosine_similarity(count_matrix, count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index and remapping like before\n",
    "metadt = metadt.reset_index()\n",
    "indices = pd.Series(metadt.index, index=metadt['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get recommendation\n",
    "get_recommendation('Toy Story', cos_sim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
